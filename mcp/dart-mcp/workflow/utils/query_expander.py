"""
ì¿¼ë¦¬ í™•ì¥ê¸°
LangExtract/vLLM â†’ ê²€ì¦ â†’ íŒŒë¼ë¯¸í„° ë°˜í™˜ì˜ ëª…í™•í•œ íë¦„
"""

from typing import Dict, List, Any
from datetime import datetime

from utils.date_parser import extract_date_range_from_query
from utils.company_validator import CompanyValidator
from workflow.utils.doc_type_mapper import DocTypeMapper
from workflow.utils.query_parser_langextract import QueryParserLangExtract
from utils.config_loader import get_openai_client

# ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ ì´ë²¤íŠ¸ íƒ€ì…
MAJOR_EVENT_TYPES = [
    'ë¶€ë„ë°œìƒ', 'ì˜ì—…ì •ì§€', 'íšŒìƒì ˆì°¨', 'í•´ì‚°ì‚¬ìœ ', 'ìœ ìƒì¦ì', 'ë¬´ìƒì¦ì', 'ìœ ë¬´ìƒì¦ì', 
    'ê°ì', 'ê´€ë¦¬ì ˆì°¨ê°œì‹œ', 'ì†Œì†¡', 'í•´ì™¸ìƒì¥ê²°ì •', 'í•´ì™¸ìƒì¥íì§€ê²°ì •', 'í•´ì™¸ìƒì¥', 
    'í•´ì™¸ìƒì¥íì§€', 'ì „í™˜ì‚¬ì±„ë°œí–‰', 'ì‹ ì£¼ì¸ìˆ˜ê¶Œë¶€ì‚¬ì±„ë°œí–‰', 'êµí™˜ì‚¬ì±„ë°œí–‰', 'ê´€ë¦¬ì ˆì°¨ì¤‘ë‹¨', 
    'ì¡°ê±´ë¶€ìë³¸ì¦ê¶Œë°œí–‰', 'ìì‚°ì–‘ìˆ˜ë„', 'íƒ€ë²•ì¸ì¦ê¶Œì–‘ë„', 'ìœ í˜•ìì‚°ì–‘ë„', 'ìœ í˜•ìì‚°ì–‘ìˆ˜', 
    'íƒ€ë²•ì¸ì¦ê¶Œì–‘ìˆ˜', 'ì˜ì—…ì–‘ë„', 'ì˜ì—…ì–‘ìˆ˜', 'ìê¸°ì£¼ì‹ì·¨ë“ì‹ íƒê³„ì•½í•´ì§€', 
    'ìê¸°ì£¼ì‹ì·¨ë“ì‹ íƒê³„ì•½ì²´ê²°', 'ìê¸°ì£¼ì‹ì²˜ë¶„', 'ìê¸°ì£¼ì‹ì·¨ë“', 'ì£¼ì‹êµí™˜', 
    'íšŒì‚¬ë¶„í• í•©ë³‘', 'íšŒì‚¬ë¶„í• ', 'íšŒì‚¬í•©ë³‘', 'ì‚¬ì±„ê¶Œì–‘ìˆ˜', 'ì‚¬ì±„ê¶Œì–‘ë„ê²°ì •'
]

# ì¦ê¶Œì‹ ê³ ì„œ íƒ€ì…
SECURITIES_TYPES = [
    'ì£¼ì‹ì˜í¬ê´„ì êµí™˜ì´ì „', 'í•©ë³‘', 'ì¦ê¶Œì˜ˆíƒì¦ê¶Œ', 'ì±„ë¬´ì¦ê¶Œ', 'ì§€ë¶„ì¦ê¶Œ', 'ë¶„í• '
]

# ì‚¬ì—…ë³´ê³ ì„œ íƒ€ì…
BUSINESS_REPORT_TYPES = [
    'ì¡°ê±´ë¶€ìë³¸ì¦ê¶Œë¯¸ìƒí™˜', 'ë¯¸ë“±ê¸°ì„ì›ë³´ìˆ˜', 'íšŒì‚¬ì±„ë¯¸ìƒí™˜', 'ë‹¨ê¸°ì‚¬ì±„ë¯¸ìƒí™˜', 'ê¸°ì—…ì–´ìŒë¯¸ìƒí™˜', 
    'ì±„ë¬´ì¦ê¶Œë°œí–‰', 'ì‚¬ëª¨ìê¸ˆì‚¬ìš©', 'ê³µëª¨ìê¸ˆì‚¬ìš©', 'ì„ì›ì „ì²´ë³´ìˆ˜ìŠ¹ì¸', 'ì„ì›ì „ì²´ë³´ìˆ˜ìœ í˜•', 
    'ì£¼ì‹ì´ìˆ˜', 'íšŒê³„ê°ì‚¬', 'ê°ì‚¬ìš©ì—­', 'íšŒê³„ê°ì‚¬ìš©ì—­ê³„ì•½', 'ì‚¬ì™¸ì´ì‚¬', 'ì‹ ì¢…ìë³¸ì¦ê¶Œë¯¸ìƒí™˜', 
    'ì¦ì', 'ë°°ë‹¹', 'ìê¸°ì£¼ì‹', 'ìµœëŒ€ì£¼ì£¼', 'ìµœëŒ€ì£¼ì£¼ë³€ë™', 'ì†Œì•¡ì£¼ì£¼', 'ì„ì›', 'ì§ì›', 
    'ì„ì›ê°œì¸ë³´ìˆ˜', 'ì„ì›ì „ì²´ë³´ìˆ˜', 'ê°œì¸ë³„ë³´ìˆ˜', 'íƒ€ë²•ì¸ì¶œì'
]


class QueryExpander:
    """ë‹¨ìˆœí™”ëœ ì¿¼ë¦¬ í™•ì¥ ë° íŒŒë¼ë¯¸í„° ìƒì„±"""
    
    def __init__(self, dart_reader=None):
        """
        Args:
            dart_reader: OpenDartReader ì¸ìŠ¤í„´ìŠ¤
        """
        # 1. LangExtract íŒŒì„œ (í•„ìˆ˜)
        self.langextract_parser = QueryParserLangExtract()
        
        # 2. ê²€ì¦ ë„êµ¬ë“¤
        self.company_validator = CompanyValidator(dart_reader)
        self.doc_mapper = DocTypeMapper(get_openai_client())
        
        # 3. ìƒì„¸ íƒ€ì… ë§¤í•‘
        self.major_event_types = MAJOR_EVENT_TYPES
        self.securities_types = SECURITIES_TYPES
        self.business_report_types = BUSINESS_REPORT_TYPES
    
    def _extract_detailed_types(self, query: str, keywords: List[str] = None) -> Dict[str, List[str]]:
        """
        ì¿¼ë¦¬ì—ì„œ ìƒì„¸ ë¬¸ì„œ íƒ€ì… ì¶”ì¶œ
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒì„¸ íƒ€ì… ë”•ì…”ë„ˆë¦¬
        """
        detailed_types = {
            "major_events": [],
            "securities": [],
            "business_reports": []
        }
        
        # ì¿¼ë¦¬ì™€ í‚¤ì›Œë“œë¥¼ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ (ê³µë°± ì œê±° ë° ì†Œë¬¸ì ë³€í™˜)
        search_text = query.lower().replace(" ", "").replace("\t", "").replace("\n", "")
        if keywords:
            search_text += "".join(keywords).lower().replace(" ", "")
        
        # ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ ì´ë²¤íŠ¸ íƒ€ì… ê²€ìƒ‰
        for event_type in self.major_event_types:
            event_normalized = event_type.lower().replace(" ", "")
            if event_normalized in search_text:
                detailed_types["major_events"].append(event_type)
        
        # ì¦ê¶Œì‹ ê³ ì„œ íƒ€ì… ê²€ìƒ‰
        for sec_type in self.securities_types:
            sec_normalized = sec_type.lower().replace(" ", "")
            if sec_normalized in search_text:
                detailed_types["securities"].append(sec_type)
        
        # ì‚¬ì—…ë³´ê³ ì„œ íƒ€ì… ê²€ìƒ‰
        for biz_type in self.business_report_types:
            biz_normalized = biz_type.lower().replace(" ", "")
            if biz_normalized in search_text:
                detailed_types["business_reports"].append(biz_type)
        
        return detailed_types
        
    async def expand_query(self, query: str) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ DART API íŒŒë¼ë¯¸í„°ë¡œ í™•ì¥
        
        íë¦„:
        1. LangExtractë¡œ íŒŒì‹±
        2. ê° ìš”ì†Œ ê²€ì¦
        3. DART API íŒŒë¼ë¯¸í„° ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Returns:
            í™•ì¥ëœ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        # ê²°ê³¼ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        params = {
            "original_query": query,
            "companies": [],
            "corp_codes": [],
            "date_range": None,
            "doc_types": [],
            "keywords": [],
            "search_params": {},
            "needs_confirmation": False
        }
        
        # Step 1: LangExtractë¡œ íŒŒì‹±
        try:
            parsed = await self.langextract_parser.parse_query(query)
        except Exception as e:
            print(f"LangExtract íŒŒì‹± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ íŒŒì‹±
            parsed = self._fallback_parse(query)
        
        # Step 2: ë‚ ì§œ ê²€ì¦ ë° ë³€í™˜
        if parsed.get("date_expressions"):
            # ì¶”ì¶œëœ ë‚ ì§œ í‘œí˜„ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
            date_text = " ".join([expr["text"] for expr in parsed["date_expressions"]])
            date_range = extract_date_range_from_query(date_text)
            
            if date_range:
                params["date_range"] = {
                    "start": date_range[0],
                    "end": date_range[1]
                }
                params["search_params"]["bgn_de"] = date_range[0]
                params["search_params"]["end_de"] = date_range[1]
        
        # Step 3: ê¸°ì—…ëª… ê²€ì¦
        companies_to_validate = []
        companies_to_validate.extend(parsed.get("companies", []))
        companies_to_validate.extend(parsed.get("stock_codes", []))
        
        for company in companies_to_validate:
            # ì¢…ëª©ì½”ë“œì¸ ê²½ìš° ë°”ë¡œ ì²˜ë¦¬
            if company.isdigit() and len(company) == 6:
                stock_result = self.company_validator.get_company_by_stock_code(company)
                if stock_result:
                    params["companies"].append(stock_result["company"])
                    params["corp_codes"].append(stock_result["corp_code"])
                    continue
            
            # ê¸°ì—…ëª… ê²€ì¦
            validation = self.company_validator.find_company(company, threshold=80)
            
            if validation["status"] in ["exact", "fuzzy"]:
                if validation["company"] and validation["corp_code"]:
                    params["companies"].append(validation["company"])
                    params["corp_codes"].append(validation["corp_code"])
                    
                    # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ í™•ì¸ í•„ìš”
                    if validation.get("score", 100) < 90:
                        params["needs_confirmation"] = True
        
        # Step 4: ë¬¸ì„œìœ í˜• ê²€ì¦ - ì „ì²´ íŒŒì‹± ê²°ê³¼ë¥¼ ì „ë‹¬
        if parsed.get("doc_types") or parsed.get("keywords"):
            # LangExtract íŒŒì‹± ê²°ê³¼ ì „ì²´ë¥¼ doc_mapperì— ì „ë‹¬
            if hasattr(self.doc_mapper, 'map_query_to_doc_types_sync'):
                mapped_types = self.doc_mapper.map_query_to_doc_types_sync(query, parsed)
            else:
                mapped_types = await self.doc_mapper.map_query_to_doc_types(query, parsed)
            
            # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ ì‚¬ìš©
            if mapped_types:
                code, confidence = mapped_types[0]
                params["doc_types"].append({
                    "code": code,
                    "name": self.doc_mapper.get_doc_type_name(code),
                    "confidence": confidence
                })
                
                # DART API íŒŒë¼ë¯¸í„° ì„¤ì •
                if confidence >= 0.5:
                    params["search_params"]["pblntf_detail_ty"] = code
        
        # Step 5: í‚¤ì›Œë“œ ì¶”ì¶œ
        if parsed.get("keywords"):
            params["keywords"] = [kw["text"] for kw in parsed["keywords"]]
        
        # Step 6: ìƒì„¸ ë¬¸ì„œ íƒ€ì… ì¶”ì¶œ
        detailed_types = self._extract_detailed_types(query, params["keywords"])
        params["detailed_types"] = detailed_types
        
        # ìƒì„¸ íƒ€ì…ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì •ë³´ë„ doc_typesì— ì¶”ê°€
        if detailed_types["major_events"]:
            params["major_events"] = detailed_types["major_events"]
        if detailed_types["securities"]:
            params["securities"] = detailed_types["securities"]
        if detailed_types["business_reports"]:
            params["business_reports"] = detailed_types["business_reports"]
        
        return params
    
    def _fallback_parse(self, query: str) -> Dict[str, Any]:
        """
        LangExtract ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ í´ë°± íŒŒì‹±
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Returns:
            ê¸°ë³¸ íŒŒì‹± ê²°ê³¼
        """
        import re
        
        result = {
            "companies": [],
            "stock_codes": [],
            "doc_types": [],
            "date_expressions": [],
            "keywords": []
        }
        
        # 6ìë¦¬ ìˆ«ìëŠ” ì¢…ëª©ì½”ë“œë¡œ ì¶”ì •
        stock_codes = re.findall(r'\b\d{6}\b', query)
        result["stock_codes"] = stock_codes
        
        # ê°„ë‹¨í•œ ë¬¸ì„œìœ í˜• ë§¤ì¹­
        doc_type_keywords = {
            "ì‚¬ì—…ë³´ê³ ì„œ": "ì‚¬ì—…ë³´ê³ ì„œ",
            "ë°˜ê¸°ë³´ê³ ì„œ": "ë°˜ê¸°ë³´ê³ ì„œ",
            "ë¶„ê¸°ë³´ê³ ì„œ": "ë¶„ê¸°ë³´ê³ ì„œ",
            "ê°ì‚¬ë³´ê³ ì„œ": "ê°ì‚¬ë³´ê³ ì„œ",
            "ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ": "ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ"
        }
        
        for keyword, name in doc_type_keywords.items():
            if keyword in query:
                result["doc_types"].append({"name": name})
                break
        
        # ë‚ ì§œ í‘œí˜„ ì¶”ì¶œ
        date_keywords = ["ì˜¬í•´", "ì‘ë…„", "ìµœê·¼", "ì–´ì œ", "ì˜¤ëŠ˜"]
        for keyword in date_keywords:
            if keyword in query:
                result["date_expressions"].append({"text": keyword})
                break
        
        return result
    
    def create_search_params(self, expanded_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        í™•ì¥ëœ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ DART API ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìƒì„±
        ê¸°ì—…ëª…ì´ ì—†ëŠ” ê²½ìš° 3ê°œì›”ì”© ë¶„í• í•˜ì—¬ ê²€ìƒ‰
        
        Args:
            expanded_query: í™•ì¥ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            DART API ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸
        """
        search_params_list = []
        base_params = expanded_query.get("search_params", {})
        
        # ê¸°ì—…ë³„ ê²€ìƒ‰
        if expanded_query["corp_codes"]:
            for corp_code in expanded_query["corp_codes"]:
                params = base_params.copy()
                params["corp_code"] = corp_code
                params["page_count"] = 100
                search_params_list.append(params)
        else:
            # ê¸°ì—…ëª… ì—†ëŠ” ì „ì²´ ê²€ìƒ‰ - 3ê°œì›” ë‹¨ìœ„ë¡œ ë¶„í• 
            from datetime import timedelta
            
            # ë‚ ì§œ ë²”ìœ„ ê²°ì •
            if "bgn_de" in base_params and "end_de" in base_params:
                start_date = datetime.strptime(base_params["bgn_de"], "%Y%m%d")
                end_date = datetime.strptime(base_params["end_de"], "%Y%m%d")
            else:
                # ë‚ ì§œ ë²”ìœ„ê°€ ì—†ìœ¼ë©´ ìµœê·¼ 3ê°œì›”
                end_date = datetime.now()
                start_date = end_date - timedelta(days=90)
            
            # ê¸°ê°„ ê³„ì‚°
            total_days = (end_date - start_date).days
            
            # 3ê°œì›”(90ì¼) ì´ˆê³¼ì‹œ ë¶„í•  ê²€ìƒ‰
            if total_days > 90:
                # 3ê°œì›”ì”© ì—­ìˆœìœ¼ë¡œ ë¶„í•  (ìµœì‹  ë°ì´í„°ë¶€í„°)
                current_end = end_date
                
                while current_end > start_date:
                    current_start = max(current_end - timedelta(days=89), start_date)  # 89ì¼ = 3ê°œì›” - 1ì¼
                    
                    params = base_params.copy()
                    params["bgn_de"] = current_start.strftime("%Y%m%d")
                    params["end_de"] = current_end.strftime("%Y%m%d")
                    params["page_count"] = 100
                    search_params_list.append(params)
                    
                    # ë‹¤ìŒ êµ¬ê°„ìœ¼ë¡œ ì´ë™ (1ì¼ ê²¹ì¹¨ ë°©ì§€)
                    current_end = current_start - timedelta(days=1)
            else:
                # 3ê°œì›” ì´í•˜ëŠ” ê·¸ëŒ€ë¡œ ê²€ìƒ‰
                params = base_params.copy()
                params["bgn_de"] = start_date.strftime("%Y%m%d") if "bgn_de" not in params else params["bgn_de"]
                params["end_de"] = end_date.strftime("%Y%m%d") if "end_de" not in params else params["end_de"]
                params["page_count"] = 100
                search_params_list.append(params)
        
        return search_params_list
    
    def format_result(self, expanded: Dict[str, Any]) -> str:
        """
        í™•ì¥ëœ ì¿¼ë¦¬ ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·
        
        Args:
            expanded: í™•ì¥ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            í¬ë§·ëœ ë¬¸ìì—´
        """
        lines = []
        lines.append(f"ğŸ“ ì›ë³¸ ì¿¼ë¦¬: {expanded['original_query']}")
        
        if expanded['companies']:
            lines.append(f"ğŸ¢ ê¸°ì—…: {', '.join(expanded['companies'])}")
            
        if expanded['date_range']:
            start = expanded['date_range']['start']
            end = expanded['date_range']['end']
            # YYYYMMDDë¥¼ YYYY-MM-DDë¡œ ë³€í™˜
            start_fmt = f"{start[:4]}-{start[4:6]}-{start[6:8]}"
            end_fmt = f"{end[:4]}-{end[4:6]}-{end[6:8]}"
            lines.append(f"ğŸ“… ê¸°ê°„: {start_fmt} ~ {end_fmt}")
            
        if expanded['doc_types']:
            doc_names = [dt['name'] for dt in expanded['doc_types']]
            lines.append(f"ğŸ“„ ë¬¸ì„œìœ í˜•: {', '.join(doc_names)}")
            
        if expanded['keywords']:
            lines.append(f"ğŸ” í‚¤ì›Œë“œ: {', '.join(expanded['keywords'][:10])}")
        
        # ìƒì„¸ ë¬¸ì„œ íƒ€ì…ë“¤
        detailed_types = expanded.get('detailed_types', {})
        if any(detailed_types.values()):
            detail_info = []
            if detailed_types.get('major_events'):
                detail_info.append(f"ì£¼ìš”ì‚¬í•­: {len(detailed_types['major_events'])}ê°œ")
            if detailed_types.get('securities'):
                detail_info.append(f"ì¦ê¶Œì‹ ê³ ì„œ: {len(detailed_types['securities'])}ê°œ")
            if detailed_types.get('business_reports'):
                detail_info.append(f"ì‚¬ì—…ë³´ê³ ì„œ: {len(detailed_types['business_reports'])}ê°œ")
            lines.append(f"ğŸ“‹ ìƒì„¸íƒ€ì…: {', '.join(detail_info)}")
        
        if expanded.get('needs_confirmation'):
            lines.append("âš ï¸ ì¼ë¶€ í•­ëª©ì€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
        return "\n".join(lines)