"""
DART ì‹¬ì¸µ ê²€ìƒ‰ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ DART ê³µì‹œë¥¼ ì‹¬ì¸µ ê²€ìƒ‰í•˜ê³  ì¢…í•©ì ì¸ ë‹µë³€ì„ ìƒì„±
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
from datetime import datetime

from workflow.utils.query_expander import QueryExpander
from workflow.utils.sufficiency_checker import SufficiencyChecker
from workflow.utils.synthesizer import DartSynthesizer
from workflow.utils.document_fetcher import DocumentFetcherV2
from workflow.utils.document_filter import DocumentFilter
from utils.logging import get_logger
from utils.cache import get_cache
from utils.config_loader import get_config, get_openai_client

# Logger ì´ˆê¸°í™”
logger = get_logger("dart_orchestrator")


class DartOrchestrator:
    """DART ì‹¬ì¸µ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, dart_api_tools):
        """
        Args:
            dart_api_tools: dart_api_tools ëª¨ë“ˆ (ë™ì  ì„í¬íŠ¸)
        """
        self.dart_api = dart_api_tools
        self.dart_reader = getattr(dart_api_tools, 'dart_reader', None)
        self.query_expander = QueryExpander(self.dart_reader)
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì„ íƒì )
        self.llm_client = get_openai_client()
        
        # LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¶©ë¶„ì„± ê²€ì‚¬ê¸°ì™€ í•©ì„±ê¸°ì— ì „ë‹¬
        self.sufficiency_checker = SufficiencyChecker(self.llm_client)
        self.synthesizer = DartSynthesizer(self.llm_client)
        
        # ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° ëª¨ë“ˆ (ê°œì„ ëœ ë²„ì „)
        self.document_fetcher = DocumentFetcherV2(dart_api_tools)
        
        # ë¬¸ì„œ í•„í„°ë§ ëª¨ë“ˆ
        self.document_filter = DocumentFilter(self.llm_client)
        
        # ìºì‹œ ì´ˆê¸°í™”
        self.cache = get_cache()
        self.config = get_config()
        
    async def search_pipeline(
        self,
        query: str,
        max_attempts: int = 3,
        max_results_per_search: int = 30
    ) -> str:
        """
        DART ì‹¬ì¸µ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            max_attempts: ìµœëŒ€ ê²€ìƒ‰ ì‹œë„ íšŸìˆ˜
            max_results_per_search: ê²€ìƒ‰ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
        """
        logger.info(f"Starting DART search pipeline for query: {query}")
        
        try:
            # 1. Query Expansion Phase
            logger.info("Phase 1: Query Expansion")
            expanded_query = await self.query_expander.expand_query(query)
            logger.info(f"Expanded query: {self.query_expander.format_result(expanded_query)}")
            
            # ê¸°ì—…ëª… í™•ì¸ì´ í•„ìš”í•œ ê²½ìš° ì²˜ë¦¬
            if expanded_query.get("needs_confirmation"):
                confirmation_result = self._handle_company_confirmation(expanded_query)
                if confirmation_result["status"] == "needs_user_input":
                    return json.dumps(confirmation_result, ensure_ascii=False, indent=2)
                # ì‚¬ìš©ìê°€ ì„ íƒí•œ ê¸°ì—…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                expanded_query = confirmation_result["updated_query"]
            
            # ìœ ì˜ë¯¸í•œ Params ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if expanded_query.get("companies") == [] and expanded_query.get("corp_codes") == [] and expanded_query.get("doc_types") == []:
                response = {
                    "query": query,
                    "answer": "Dart ê³µì‹œ ê´€ë ¨ ë‹µë³€ì´ í•„ìš”í•˜ì‹œêµ°ìš”. 'ì‚¼ì„±ì „ì', 'ì˜ì—…ì´ìµ ê³µì‹œ', 'ìœ ìƒì¦ì' ì™€ ê°™ì´ êµ¬ì²´ì ì¸ ê¸°ì—…ëª…ì´ë‚˜ ê³µì‹œ ê´€ë ¨ ìš©ì–´ë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ ì£¼ì‹œë©´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "summary": {
                        "total_documents": 0,
                        "date_range": "",
                        "companies": [],
                        "confidence": 0.0
                    },
                    "documents": [],
                    "metadata": {
                        "synthesized_at": datetime.now().isoformat(),
                        "sufficiency": False
                    }
                }
                return json.dumps(response, ensure_ascii=False, indent=2)

            # 2. Search Phase
            logger.info("Phase 2: Search Execution")
            search_results = await self._execute_searches(expanded_query, max_results_per_search)
            
            if not search_results:
                logger.warning("No search results found")
                return json.dumps({
                    "status": "no_results",
                    "query": query,
                    "message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "expanded_query": expanded_query
                }, ensure_ascii=False)
            
            logger.info(f"Found {len(search_results)} total results")
            
            # 3. Document Filtering Phase
            logger.info("Phase 3: Document Filtering")
            filtered_results = await self.document_filter.filter_documents(query, search_results, expanded_query)
            logger.info(f"Filtered to {len(filtered_results)} relevant documents from {len(search_results)}")
            
            # 4. Document Processing Phase
            logger.info("Phase 4: Document Processing")
            processed_docs = await self._process_documents(filtered_results, expanded_query)
            
            # 5. Synthesis Phase (ì£¼ì„ ì²˜ë¦¬ë¨ - í…ŒìŠ¤íŠ¸ìš©)
            # logger.info("Phase 5: Synthesis")
            # synthesis_result = await self.synthesizer.synthesize(
            #     query,
            #     processed_docs,
            #     expanded_query,
            #     {
            #         "total_searched": len(search_results),
            #         "total_filtered": len(filtered_results),
            #         "total_processed": len(processed_docs)
            #     }
            # )
            
            # ìºì‹œ í†µê³„ ë¡œê¹…
            cache_stats = self.cache.get_stats()
            logger.info(f"Cache stats: {cache_stats}")
            
            # í…ŒìŠ¤íŠ¸ìš©: processed_docs ì§ì ‘ ë°˜í™˜
            test_result = {
                "status": "success",
                "query": query,
                "total_searched": len(search_results),
                "total_filtered": len(filtered_results),
                "total_processed": len(processed_docs),
                "processed_docs": processed_docs
            }
            return json.dumps(test_result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"Pipeline error: {str(e)}")
            return json.dumps({
                "status": "error",
                "query": query,
                "error": str(e)
            }, ensure_ascii=False)
    
    async def _execute_searches(
        self,
        expanded_query: Dict[str, Any],
        max_results: int
    ) -> List[Dict[str, Any]]:
        """
        í™•ì¥ëœ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ DART ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            expanded_query: í™•ì¥ëœ ì¿¼ë¦¬ ì •ë³´
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        all_results = []
        search_params_list = self.query_expander.create_search_params(expanded_query)
        
        # ê¸°ì—…ì½”ë“œ ì—†ëŠ” ê²½ìš° ë¡¤ë§ ê²€ìƒ‰ ì•Œë¦¼
        if len(search_params_list) > 1 and not expanded_query.get("corp_codes"):
            logger.info(f"ğŸ”„ ê¸°ì—…ëª… ì—†ëŠ” ê²€ìƒ‰: {len(search_params_list)}ê°œ ê¸°ê°„ìœ¼ë¡œ ë¶„í•  (3ê°œì›”ì”© ë¡¤ë§)")
            for i, params in enumerate(search_params_list, 1):
                start = params.get('bgn_de', '')
                end = params.get('end_de', '')
                if start and end:
                    start_fmt = f"{start[:4]}-{start[4:6]}-{start[6:8]}"
                    end_fmt = f"{end[:4]}-{end[4:6]}-{end[6:8]}"
                    logger.info(f"  ğŸ“… êµ¬ê°„ {i}: {start_fmt} ~ {end_fmt}")
        
        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        if len(search_params_list) > 1 and expanded_query.get("search_strategy", {}).get("parallel_search"):
            logger.info(f"Executing {len(search_params_list)} parallel searches")
            
            tasks = []
            for params in search_params_list:
                tasks.append(self._search_disclosures(params))
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if not isinstance(result, Exception):
                    all_results.extend(result)
        else:
            # ìˆœì°¨ ê²€ìƒ‰
            for i, params in enumerate(search_params_list, 1):
                period_info = f"[{i}/{len(search_params_list)}]" if len(search_params_list) > 1 else ""
                logger.info(f"Executing search {period_info}: {params.get('bgn_de', 'N/A')} ~ {params.get('end_de', 'N/A')}")
                logger.info(f"  â†’ Search params: {params}")  # ì „ì²´ íŒŒë¼ë¯¸í„° ë¡œê¹…
                results = await self._search_disclosures(params)
                all_results.extend(results)
                
                logger.info(f"  â†’ {len(results)}ê±´ ê²€ìƒ‰, ëˆ„ì  {len(all_results)}ê±´")
                
                # ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
                if len(all_results) >= max_results:
                    logger.info(f"ì¶©ë¶„í•œ ê²°ê³¼ ìˆ˜ì§‘ ({len(all_results)}ê±´), ê²€ìƒ‰ ì¤‘ë‹¨")
                    break
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        logger.info(f"Before deduplication: {len(all_results)} results")
        unique_results = self._deduplicate_results(all_results)
        logger.info(f"After deduplication: {len(unique_results)} results")
        
        # ê²°ê³¼ ìˆ˜ ì œí•œ
        return unique_results[:max_results]
    
    async def _search_disclosures(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        DART ê³µì‹œ ê²€ìƒ‰ ì‹¤í–‰ (ìºì‹± ì ìš©)
        
        Args:
            params: ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ìºì‹œ í‚¤ ìƒì„±ìš© íŒŒë¼ë¯¸í„°
        cache_params = {
            "company": params.get("corp_code"),
            "start_date": self._format_date_for_api(params.get("bgn_de")),
            "end_date": self._format_date_for_api(params.get("end_de")),
            "pblntf_detail_ty": params.get("pblntf_detail_ty")  # ìƒì„¸ìœ í˜• ê·¸ëŒ€ë¡œ ì‚¬ìš©
        }
        
        # Phase 2 Search Execution - íŒŒë¼ë¯¸í„° ë¡œê¹… (Phase 1ì—ì„œ ì„ íƒí•œ ë¬¸ì„œ ìœ í˜• í™•ì¸)
        logger.info(f"Phase 2 Search Execution - cache_params: {cache_params}")
        logger.info(f"  â†’ pblntf_detail_ty (ë¬¸ì„œìœ í˜•): {params.get('pblntf_detail_ty', 'None')}")
        
        # ìºì‹œ í™•ì¸
        cached_result = await self.cache.get("search_company_disclosures", cache_params)
        if cached_result is not None:
            logger.debug(f"Cache hit for search params: {cache_params}")
            return cached_result
        
        try:
            # dart_api_toolsì˜ search_company_disclosures í•¨ìˆ˜ í˜¸ì¶œ
            result_json = await self.dart_api.search_company_disclosures(
                **cache_params
            )
            
            # JSON íŒŒì‹±
            if isinstance(result_json, str):
                result_data = json.loads(result_json)
            else:
                result_data = result_json
            
            # ì—ëŸ¬ ì²´í¬
            if "error" in result_data:
                logger.error(f"Search error: {result_data['error']}")
                return []
            
            # ê²°ê³¼ ì¶”ì¶œ
            results = []
            if isinstance(result_data, list):
                results = result_data
            elif isinstance(result_data, dict) and "result" in result_data:
                if result_data["result"] == "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.":
                    results = []
                else:
                    results = result_data.get("list", [])
            
            # ìºì‹œì— ì €ì¥
            if results:
                await self.cache.set("search_company_disclosures", cache_params, results)
                logger.debug(f"Cached {len(results)} search results")
            
            return results
            
        except Exception as e:
            logger.error(f"Search execution error: {str(e)}")
            return []
    
    def _format_date_for_api(self, date_str: Optional[str]) -> Optional[str]:
        """YYYYMMDDë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not date_str or len(date_str) != 8:
            return None
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
    
    def _extract_kind_from_detail_type(self, detail_type: Optional[str]) -> Optional[str]:
        """ìƒì„¸ìœ í˜•ì—ì„œ ê³µì‹œì¢…ë¥˜ ì¶”ì¶œ"""
        if not detail_type:
            return None
            
        # ì²« ê¸€ìê°€ ê³µì‹œì¢…ë¥˜ë¥¼ ë‚˜íƒ€ëƒ„
        first_chars = set()
        for dtype in detail_type.split(","):
            if dtype:
                first_chars.add(dtype[0])
        
        # ë‹¨ì¼ ì¢…ë¥˜ë§Œ ìˆìœ¼ë©´ ë°˜í™˜
        if len(first_chars) == 1:
            return list(first_chars)[0]
            
        return None
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        seen = set()
        unique = []
        
        for result in results:
            # rcept_noë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            key = result.get("rcept_no") or result.get("rcp_no")
            if key:
                if key not in seen:
                    seen.add(key)
                    unique.append(result)
            else:
                # ì ‘ìˆ˜ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ë””ë²„ê¹…ìš© ë¡œê·¸
                logger.warning(f"No rcept_no found in result: {list(result.keys())}")
                # ì ‘ìˆ˜ë²ˆí˜¸ê°€ ì—†ì–´ë„ ê²°ê³¼ì— í¬í•¨ (ë‹¤ë¥¸ ê³ ìœ  ì‹ë³„ì ì‹œë„)
                backup_key = f"{result.get('corp_name', '')}-{result.get('report_nm', '')}-{result.get('rcept_dt', '')}"
                if backup_key not in seen:
                    seen.add(backup_key)
                    unique.append(result)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        unique.sort(key=lambda x: x.get("rcept_dt", ""), reverse=True)
        
        return unique
    
    async def _process_documents(
        self,
        filtered_results: List[Dict[str, Any]],
        expanded_query: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        í•„í„°ë§ëœ ë¬¸ì„œ ì²˜ë¦¬ ë° ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        
        Args:
            filtered_results: í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼
            expanded_query: í™•ì¥ëœ ì¿¼ë¦¬ (pblntf_detail_ty ì •ë³´ í¬í•¨)
            
        Returns:
            ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ë‚´ìš© í¬í•¨)
        """
        if not filtered_results:
            return []
        
        report_type = expanded_query.get("doc_types")[0]['code']
        
        processed = []
        
        # 1. ê¸°ë³¸ ì •ë³´ ì²˜ë¦¬ (ì´ë¯¸ í•„í„°ë§ë˜ê³  ì •ë ¬ëœ ë¬¸ì„œë“¤)
        for idx, result in enumerate(filtered_results):
            rcept_no = result.get("rcept_no") or result.get("rcp_no")

            doc_info = {
                "index": idx + 1,
                "rcept_no": rcept_no,
                "corp_name": result.get("corp_name"),
                "report_nm": result.get("report_nm"),
                "rcept_dt": result.get("rcept_dt"),
                "flr_nm": result.get("flr_nm"),  # ì œì¶œì¸
                "summary": self._extract_summary(result),
                "content": None,  # ë‚˜ì¤‘ì— ì±„ìš¸ ì˜ˆì •
                "key_info": {},
                "report_type": report_type,  # expanded_queryì—ì„œ ê°€ì ¸ì˜´
            }
            
            processed.append(doc_info)
        
        # 2. í•„í„°ë§ëœ ë¬¸ì„œì˜ ì‹¤ì œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        docs_to_fetch = processed  # ì´ë¯¸ í•„í„°ë§ëœ ë¬¸ì„œë§Œ ì²˜ë¦¬
        
        if docs_to_fetch:
            logger.info(f"Fetching content for top {len(docs_to_fetch)} documents")
            
            # ê° ë¬¸ì„œì— corp_code ì¶”ê°€ (company_validator ì‚¬ìš©)
            for doc in docs_to_fetch:
                if doc.get("corp_name") and not doc.get("corp_code"):
                    # íšŒì‚¬ëª…ìœ¼ë¡œ corp_code ì¡°íšŒ (query_expanderì˜ company_validator í™œìš©)
                    validation = self.query_expander.company_validator.find_company(doc["corp_name"])
                    if validation["status"] in ["exact", "fuzzy"] and validation.get("corp_code"):
                        doc["corp_code"] = validation["corp_code"]
            
            # ê°œì„ ëœ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ìƒì„¸ íƒ€ì… ì „ë‹¬)
            detailed_types = expanded_query.get("detailed_types", {})
            fetched_contents = await self.document_fetcher.fetch_multiple_documents(
                docs_to_fetch,
                max_concurrent=3,
                detailed_types=detailed_types
            )
            
            # ê°€ì ¸ì˜¨ ë‚´ìš©ì„ ë¬¸ì„œ ì •ë³´ì— ë³‘í•©
            for i, fetched in enumerate(fetched_contents):
                if i < len(processed):
                    doc = processed[i]
                    
                    # fetchedì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë“  í•„ë“œë¥¼ docì— ë³‘í•©
                    # ê¸°ì¡´ docì˜ í•„ë“œë¥¼ ìœ ì§€í•˜ë©´ì„œ fetchedì˜ ìƒˆ í•„ë“œ ì¶”ê°€
                    for key, value in fetched.items():
                        if value is not None:  # Noneì´ ì•„ë‹Œ ê°’ë§Œ ì—…ë°ì´íŠ¸
                            doc[key] = value
                    
                    if fetched.get("error"):
                        logger.warning(f"Error fetching content for {doc.get('rcept_no')}: {fetched['error']}")
        
        return processed
    
    
    def _extract_summary(self, result: Dict[str, Any]) -> str:
        """ë¬¸ì„œ ìš”ì•½ ì¶”ì¶œ"""
        parts = []
        
        if result.get("report_nm"):
            parts.append(result["report_nm"])
            
        if result.get("rm"):  # ë¹„ê³ 
            parts.append(f"ë¹„ê³ : {result['rm']}")
            
        return " | ".join(parts) if parts else "ìš”ì•½ ì—†ìŒ"
    
    async def _synthesize_response(
        self,
        query: str,
        processed_docs: List[Dict[str, Any]],
        expanded_query: Dict[str, Any]
    ) -> str:
        """
        ìµœì¢… ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            processed_docs: ì²˜ë¦¬ëœ ë¬¸ì„œ
            expanded_query: í™•ì¥ëœ ì¿¼ë¦¬
            
        Returns:
            JSON í˜•ì‹ì˜ ìµœì¢… ì‘ë‹µ
        """
        # ì£¼ìš” ë¬¸ì„œ ì„ íƒ (ìƒìœ„ 10ê°œ)
        top_docs = processed_docs[:10]
        
        # í†µê³„ ì •ë³´ ìƒì„±
        stats = {
            "total_results": len(processed_docs),
            "companies": list(set(doc["corp_name"] for doc in processed_docs if doc.get("corp_name"))),
            "date_range": {
                "earliest": min((doc["rcept_dt"] for doc in processed_docs if doc.get("rcept_dt")), default=""),
                "latest": max((doc["rcept_dt"] for doc in processed_docs if doc.get("rcept_dt")), default="")
            },
            "report_types": {}
        }
        
        # ë³´ê³ ì„œ ìœ í˜•ë³„ ì§‘ê³„
        for doc in processed_docs:
            report_type = doc.get("report_nm", "ê¸°íƒ€")
            stats["report_types"][report_type] = stats["report_types"].get(report_type, 0) + 1
        
        # ì‘ë‹µ êµ¬ì„±
        response = {
            "status": "success",
            "query": query,
            "expanded_query": {
                "companies": expanded_query["companies"],
                "date_range": expanded_query["date_range"],
                "doc_types": expanded_query["doc_types"][:3],
                "keywords": expanded_query["keywords"][:5]
            },
            "statistics": stats,
            "top_results": top_docs,
            "summary": self._generate_summary(query, top_docs, stats),
            "timestamp": datetime.now().isoformat()
        }
        
        return json.dumps(response, ensure_ascii=False, indent=2)
    
    def _handle_company_confirmation(self, expanded_query: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê¸°ì—…ëª… í™•ì¸ì´ í•„ìš”í•œ ê²½ìš° ì²˜ë¦¬
        
        Args:
            expanded_query: í™•ì¥ëœ ì¿¼ë¦¬ ì •ë³´
            
        Returns:
            í™•ì¸ ê²°ê³¼ ë˜ëŠ” ì‚¬ìš©ì ì…ë ¥ ìš”ì²­
        """
        validation_info = expanded_query.get("company_validation", {})
        validation_results = validation_info.get("validation_results", [])
        
        # í™•ì¸ì´ í•„ìš”í•œ ê¸°ì—…ë“¤ ìˆ˜ì§‘
        ambiguous_companies = []
        for i, result in enumerate(validation_results):
            if result["needs_confirmation"]:
                original_query = validation_info["original_queries"][i]
                ambiguous_companies.append({
                    "original_query": original_query,
                    "candidates": result["candidates"],
                    "status": result["status"]
                })
        
        if ambiguous_companies:
            # ì‚¬ìš©ìì—ê²Œ í™•ì¸ ìš”ì²­
            logger.info(f"Found {len(ambiguous_companies)} ambiguous company names")
            
            response = {
                "status": "needs_user_input",
                "type": "company_confirmation",
                "query": expanded_query["original_query"],
                "message": "ì…ë ¥í•˜ì‹  ê¸°ì—…ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "ambiguous_companies": ambiguous_companies,
                "instructions": "ê° ê¸°ì—…ì— ëŒ€í•´ ì •í™•í•œ ê¸°ì—…ëª…ì„ ì„ íƒí•˜ê±°ë‚˜, ì›í•˜ëŠ” ê¸°ì—…ì´ ì—†ìœ¼ë©´ 'ì œì™¸'ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
            }
            
            # í›„ë³´ ê¸°ì—… ì •ë³´ í¬í•¨
            for item in ambiguous_companies:
                logger.info(f"  '{item['original_query']}' í›„ë³´:")
                for candidate in item["candidates"][:3]:
                    if 'score' in candidate:
                        logger.info(f"    - {candidate.get('name', candidate.get('corp_name', 'Unknown'))} (ì ìˆ˜: {candidate['score']})")
                    else:
                        logger.info(f"    - {candidate.get('corp_name', 'Unknown')} (ìœ ì‚¬ë„: {candidate.get('similarity', 0):.2f})")
            
            return response
        
        # í™•ì¸ ë¶ˆí•„ìš” - ì›ë˜ ì¿¼ë¦¬ ë°˜í™˜
        return {
            "status": "confirmed",
            "updated_query": expanded_query
        }
    
    def _generate_summary(
        self,
        query: str,
        top_docs: List[Dict[str, Any]],
        stats: Dict[str, Any]
    ) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        lines = []
        
        # ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
        if query:
            lines.append(f"ê²€ìƒ‰ì–´: {query}")
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        lines.append(f"ì´ {stats['total_results']}ê±´ì˜ ê³µì‹œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # ê¸°ì—…ë³„ ìš”ì•½
        if stats["companies"]:
            companies_str = ", ".join(stats["companies"][:5])
            lines.append(f"ê´€ë ¨ ê¸°ì—…: {companies_str}")
        
        # ê¸°ê°„ ìš”ì•½
        if stats["date_range"]["earliest"] and stats["date_range"]["latest"]:
            lines.append(f"ê³µì‹œ ê¸°ê°„: {stats['date_range']['earliest']} ~ {stats['date_range']['latest']}")
        
        # ì£¼ìš” ë³´ê³ ì„œ ìœ í˜•
        if stats["report_types"]:
            top_types = sorted(stats["report_types"].items(), key=lambda x: x[1], reverse=True)[:3]
            types_str = ", ".join([f"{t[0]}({t[1]}ê±´)" for t in top_types])
            lines.append(f"ì£¼ìš” ê³µì‹œ: {types_str}")
        
        # ìµœì‹  ê³µì‹œ í•˜ì´ë¼ì´íŠ¸
        if top_docs:
            lines.append("\nìµœê·¼ ì£¼ìš” ê³µì‹œ:")
            for doc in top_docs[:3]:
                lines.append(f"- [{doc['corp_name']}] {doc['report_nm']} ({doc['rcept_dt']})")
        
        return "\n".join(lines)
    
    def _map_detail_type_to_report_type(self, pblntf_detail_ty: str, report_nm: str) -> Optional[str]:
        """pblntf_detail_tyì™€ report_nmì„ ê¸°ë°˜ìœ¼ë¡œ report_type ë§¤í•‘"""
        if not pblntf_detail_ty:
            # pblntf_detail_tyê°€ ì—†ìœ¼ë©´ report_nmì—ì„œ ì¶”ë¡ 
            return self.document_fetcher._infer_report_type({"report_nm": report_nm})
        
        # pblntf_detail_tyì˜ ì²« ê¸€ìë¡œ ëŒ€ë¶„ë¥˜ íŒë‹¨
        if pblntf_detail_ty.startswith("A"):
            # ì •ê¸°ë³´ê³ ì„œ
            if "ì‚¬ì—…ë³´ê³ ì„œ" in report_nm:
                return "A001"
            elif "ë°˜ê¸°ë³´ê³ ì„œ" in report_nm:
                return "A002"
            elif "ë¶„ê¸°ë³´ê³ ì„œ" in report_nm:
                return "A003"
            else:
                return "A001"  # ê¸°ë³¸ê°’
        elif pblntf_detail_ty.startswith("B"):
            # ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ
            if "ì£¼ìš”ê²½ì˜" in report_nm:
                return "B002"
            elif "ìµœëŒ€ì£¼ì£¼" in report_nm:
                return "B003"
            else:
                return "B001"  # ê¸°ë³¸ê°’
        elif pblntf_detail_ty.startswith("C"):
            # ì¦ê¶Œì‹ ê³ ì„œ
            if "ì±„ë¬´" in report_nm or "ì±„ê¶Œ" in report_nm:
                return "C002"
            elif "íŒŒìƒ" in report_nm:
                return "C003"
            else:
                return "C001"  # ê¸°ë³¸ê°’
        elif pblntf_detail_ty.startswith("D"):
            # ì§€ë¶„ê³µì‹œ
            if "ì„ì›" in report_nm and "ì£¼ì£¼" in report_nm:
                return "D002"
            elif "ì˜ê²°ê¶Œ" in report_nm:
                return "D003"
            elif "ê³µê°œë§¤ìˆ˜" in report_nm:
                return "D004"
            else:
                return "D001"  # ê¸°ë³¸ê°’
        
        # ê¸°ë³¸ì ìœ¼ë¡œëŠ” document_fetcherì˜ ì¶”ë¡  ë¡œì§ ì‚¬ìš©
        return self.document_fetcher._infer_report_type({"report_nm": report_nm})


async def dart_research_pipeline(query: str, dart_api_tools=None) -> str:
    """
    DART ì‹¬ì¸µ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì™¸ë¶€ í˜¸ì¶œìš©)
    
    Args:
        query: ì‚¬ìš©ì ì¿¼ë¦¬
        dart_api_tools: dart_api_tools ëª¨ë“ˆ
        
    Returns:
        JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
    """
    if dart_api_tools is None:
        # ë™ì  ì„í¬íŠ¸
        try:
            from tools import dart_api_tools
        except ImportError:
            return json.dumps({
                "status": "error",
                "error": "dart_api_tools module not found"
            }, ensure_ascii=False)
    
    orchestrator = DartOrchestrator(dart_api_tools)
    return await orchestrator.search_pipeline(query)