"""
LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï Ïú†Ìã∏Î¶¨Ìã∞
Î©îÏù∏ LLMÍ≥º Ï∂îÏ∂úÏö© LLMÏùÑ Íµ¨Î∂ÑÌïòÏó¨ Í¥ÄÎ¶¨
"""

import os
from typing import Optional, Dict, Any, Tuple
from dotenv import load_dotenv
from openai import OpenAI

# Ï°∞Í±¥Î∂Ä import
try:
    import langextract as lx
    LANGEXTRACT_AVAILABLE = True
except ImportError:
    LANGEXTRACT_AVAILABLE = False


class LLMClientConfig:
    """LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï Í¥ÄÎ¶¨"""
    
    def __init__(self, use_extraction_llm: bool = True):
        """
        Args:
            use_extraction_llm: TrueÎ©¥ Ï∂îÏ∂úÏö© LLM ÏÑ§Ï†ï, FalseÎ©¥ Î©îÏù∏ LLM ÏÑ§Ï†ï
        """
        # ÌôòÍ≤ΩÎ≥ÄÏàò Î°úÎìú
        load_dotenv('.env')
        
        self.use_extraction_llm = use_extraction_llm
        self.config = self._load_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú ÏÑ§Ï†ï Î°úÎìú"""
        config = {}
        
        if self.use_extraction_llm:
            # Ï∂îÏ∂úÏö© LLM ÏÑ§Ï†ï Î°úÎìú
            config = self._load_extraction_config()
        else:
            # Î©îÏù∏ LLM ÏÑ§Ï†ï Î°úÎìú
            config = self._load_main_config()
        
        return config
    
    def _load_extraction_config(self) -> Dict[str, Any]:
        """Ï∂îÏ∂úÏö© LLM ÏÑ§Ï†ï Î°úÎìú (ÏøºÎ¶¨ ÌååÏã±, Ï†ïÎ≥¥ Ï∂îÏ∂ú)"""
        config = {}
        
        # Ïö∞ÏÑ†ÏàúÏúÑ: vLLM > LangExtract > Main LLM > Ollama
        
        if os.getenv('EXTRACTION_USE_VLLM', 'false').lower() == 'true':
            # vLLM ÏÑúÎ≤Ñ ÏÇ¨Ïö©
            config['mode'] = 'vllm'
            config['base_url'] = os.getenv('EXTRACTION_VLLM_BASE_URL', 'http://localhost:8000/v1')
            config['api_key'] = os.getenv('EXTRACTION_VLLM_API_KEY', 'dummy-key')
            config['model_name'] = os.getenv('EXTRACTION_VLLM_MODEL_NAME', 'meta-llama/Llama-3.1-8B-Instruct')
            print(f"üì° Ï∂îÏ∂úÏö© vLLM ÏÑúÎ≤Ñ: {config['base_url']}")
            
        elif os.getenv('EXTRACTION_USE_LANGEXTRACT', 'false').lower() == 'true':
            # LangExtract (Gemini) ÏÇ¨Ïö©
            config['mode'] = 'langextract'
            config['api_key'] = os.getenv('LANGEXTRACT_API_KEY')
            config['model_name'] = os.getenv('LANGEXTRACT_MODEL', 'gemini-2.0-flash-exp')
            
            if LANGEXTRACT_AVAILABLE and config['api_key']:
                print(f"‚úÖ LangExtract Î™®Îìú: {config['model_name']}")
            else:
                if not LANGEXTRACT_AVAILABLE:
                    print("‚ö†Ô∏è LangExtractÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
                if not config['api_key']:
                    print("‚ö†Ô∏è LANGEXTRACT_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
                # Î©îÏù∏ LLMÏúºÎ°ú Ìè¥Î∞±
                return self._load_main_config()
                
        elif os.getenv('EXTRACTION_USE_MAIN_LLM', 'false').lower() == 'true':
            # Î©îÏù∏ LLM ÏÑ§Ï†ï Ïû¨ÏÇ¨Ïö©
            return self._load_main_config()
            
        elif os.getenv('EXTRACTION_USE_OLLAMA', 'false').lower() == 'true':
            # Ollama ÏÇ¨Ïö©
            config['mode'] = 'ollama'
            config['base_url'] = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
            config['model_name'] = os.getenv('OLLAMA_MODEL', 'llama3.1:8b')
            print(f"ü¶ô Ollama Î™®Îìú: {config['model_name']}")
            
        else:
            # Í∏∞Î≥∏Í∞í: Î©îÏù∏ LLM ÏÇ¨Ïö©
            print("‚ÑπÔ∏è Ï∂îÏ∂úÏö© LLM ÎØ∏ÏÑ§Ï†ï, Î©îÏù∏ LLM ÏÇ¨Ïö©")
            return self._load_main_config()
        
        return config
    
    def _load_main_config(self) -> Dict[str, Any]:
        """Î©îÏù∏ LLM ÏÑ§Ï†ï Î°úÎìú (Î¨∏ÏÑú Î∂ÑÏÑù, ÏöîÏïΩ Îì±)"""
        config = {}
        provider = os.getenv('LLM_PROVIDER', 'none')
        
        if provider == 'openai':
            config['mode'] = 'openai'
            config['api_key'] = os.getenv('OPENAI_API_KEY')
            config['model_name'] = os.getenv('LLM_MODEL', 'gpt-3.5-turbo')
            config['temperature'] = float(os.getenv('LLM_TEMPERATURE', '0.3'))
            config['max_tokens'] = int(os.getenv('LLM_MAX_TOKENS', '1000'))
            
            if config['api_key']:
                print(f"ü§ñ OpenAI Î™®Îìú: {config['model_name']}")
            else:
                print("‚ö†Ô∏è OPENAI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
                
        elif provider == 'vllm':
            config['mode'] = 'vllm'
            config['base_url'] = os.getenv('LLM_BASE_URL', 'http://localhost:8000/v1')
            config['api_key'] = os.getenv('LLM_API_KEY', 'dummy-key')
            config['model_name'] = os.getenv('LLM_MODEL', 'meta-llama/Llama-3.1-8B-Instruct')
            config['temperature'] = float(os.getenv('LLM_TEMPERATURE', '0.3'))
            config['max_tokens'] = int(os.getenv('LLM_MAX_TOKENS', '1000'))
            print(f"üì° Î©îÏù∏ vLLM ÏÑúÎ≤Ñ: {config['base_url']}")
            
        else:
            config['mode'] = 'none'
            print("‚ÑπÔ∏è LLMÏù¥ ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
        
        return config
    
    def get_openai_client(self) -> Optional[OpenAI]:
        """OpenAI Ìò∏Ìôò ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î∞òÌôò"""
        mode = self.config.get('mode')
        
        if mode == 'openai':
            return OpenAI(api_key=self.config.get('api_key'))
        elif mode == 'vllm':
            return OpenAI(
                base_url=self.config.get('base_url'),
                api_key=self.config.get('api_key')
            )
        elif mode == 'ollama':
            # OllamaÎèÑ OpenAI Ìò∏Ìôò API Ï†úÍ≥µ
            return OpenAI(
                base_url=f"{self.config.get('base_url')}/v1",
                api_key='ollama'  # OllamaÎäî API ÌÇ§ Î∂àÌïÑÏöî
            )
        return None
    
    def get_langextract_config(self) -> Tuple[str, Optional[str]]:
        """LangExtract ÏÑ§Ï†ï Î∞òÌôò (model_id, api_key)"""
        if self.config.get('mode') == 'langextract':
            return self.config.get('model_name'), self.config.get('api_key')
        return None, None
    
    def is_vllm_mode(self) -> bool:
        """vLLM Î™®Îìú Ïó¨Î∂Ä"""
        return self.config.get('mode') == 'vllm'
    
    def is_langextract_available(self) -> bool:
        """LangExtract ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä"""
        return (
            LANGEXTRACT_AVAILABLE and 
            self.config.get('mode') == 'langextract' and 
            self.config.get('api_key') is not None
        )
    
    def get_model_name(self) -> str:
        """ÌòÑÏû¨ Î™®Îç∏Î™Ö Î∞òÌôò"""
        return self.config.get('model_name', 'unknown')
    
    def get_mode(self) -> str:
        """ÌòÑÏû¨ Î™®Îìú Î∞òÌôò"""
        return self.config.get('mode', 'unknown')
    
    def get_temperature(self) -> float:
        """Ïò®ÎèÑ ÏÑ§Ï†ï Î∞òÌôò"""
        return self.config.get('temperature', 0.3)
    
    def get_max_tokens(self) -> int:
        """ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò Î∞òÌôò"""
        return self.config.get('max_tokens', 500)


# Ïã±Í∏ÄÌÜ§ Ïù∏Ïä§ÌÑ¥Ïä§
_main_config = None
_extraction_config = None

def get_main_llm_config() -> LLMClientConfig:
    """Î©îÏù∏ LLM ÏÑ§Ï†ï Î∞òÌôò"""
    global _main_config
    if _main_config is None:
        _main_config = LLMClientConfig(use_extraction_llm=False)
    return _main_config

def get_extraction_llm_config() -> LLMClientConfig:
    """Ï∂îÏ∂úÏö© LLM ÏÑ§Ï†ï Î∞òÌôò"""
    global _extraction_config
    if _extraction_config is None:
        _extraction_config = LLMClientConfig(use_extraction_llm=True)
    return _extraction_config

# Í∏∞Î≥∏Í∞í: Ï∂îÏ∂úÏö© LLM
def get_llm_config() -> LLMClientConfig:
    """Í∏∞Î≥∏ LLM ÏÑ§Ï†ï (Ï∂îÏ∂úÏö©) Î∞òÌôò"""
    return get_extraction_llm_config()


def create_chat_completion(
    messages: list,
    model: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    config: Optional[LLMClientConfig] = None,
    use_main_llm: bool = False
) -> Optional[str]:
    """
    ÌÜµÌï© Ï±ÑÌåÖ ÏôÑÏÑ± Ìï®Ïàò
    
    Args:
        messages: OpenAI ÌòïÏãù Î©îÏãúÏßÄ Î¶¨Ïä§Ìä∏
        model: Î™®Îç∏Î™Ö (NoneÏùº Í≤ΩÏö∞ ÏÑ§Ï†ïÏóêÏÑú Í∞ÄÏ†∏Ïò¥)
        temperature: ÏÉùÏÑ± Ïò®ÎèÑ
        max_tokens: ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò
        config: LLM ÏÑ§Ï†ï (NoneÏùº Í≤ΩÏö∞ Í∏∞Î≥∏ ÏÑ§Ï†ï ÏÇ¨Ïö©)
        use_main_llm: TrueÎ©¥ Î©îÏù∏ LLM ÏÇ¨Ïö©, FalseÎ©¥ Ï∂îÏ∂úÏö© LLM ÏÇ¨Ïö©
        
    Returns:
        ÏÉùÏÑ±Îêú ÌÖçÏä§Ìä∏ ÎòêÎäî None
    """
    if config is None:
        config = get_main_llm_config() if use_main_llm else get_extraction_llm_config()
    
    client = config.get_openai_client()
    if client:
        try:
            response = client.chat.completions.create(
                model=model or config.get_model_name(),
                messages=messages,
                temperature=temperature or config.get_temperature(),
                max_tokens=max_tokens or config.get_max_tokens()
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"LLM Ìò∏Ï∂ú Ïã§Ìå®: {e}")
            return None
    
    return None


# LangExtract Ìó¨Ìçº Ìï®ÏàòÎì§
def create_langextract_example(
    text: str,
    extractions: list
) -> Any:
    """LangExtract ÏòàÏ†ú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"""
    if not LANGEXTRACT_AVAILABLE:
        return None
    
    extraction_objects = []
    for ext in extractions:
        extraction_objects.append(
            lx.data.Extraction(
                extraction_class=ext.get('class'),
                extraction_text=ext.get('text'),
                attributes=ext.get('attributes', {})
            )
        )
    
    return lx.data.ExampleData(
        text=text,
        extractions=extraction_objects
    )